{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gender': 'female', 'age': 21.0, 'emotion': {'anger': 0.001, 'contempt': 0.005, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.004, 'neutral': 0.983, 'sadness': 0.004, 'surprise': 0.003}}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gender': 'female', 'age': 21.0, 'emotion': {'anger': 0.0, 'contempt': 0.001, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.002, 'neutral': 0.984, 'sadness': 0.01, 'surprise': 0.003}}\n",
      "{'gender': 'female', 'age': 22.0, 'emotion': {'anger': 0.0, 'contempt': 0.001, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.001, 'neutral': 0.983, 'sadness': 0.013, 'surprise': 0.001}}\n",
      "{'gender': 'female', 'age': 21.0, 'emotion': {'anger': 0.0, 'contempt': 0.004, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.004, 'neutral': 0.976, 'sadness': 0.016, 'surprise': 0.001}}\n",
      "{'gender': 'female', 'age': 22.0, 'emotion': {'anger': 0.0, 'contempt': 0.001, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.001, 'neutral': 0.979, 'sadness': 0.017, 'surprise': 0.001}}\n",
      "{'gender': 'female', 'age': 21.0, 'emotion': {'anger': 0.0, 'contempt': 0.001, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.001, 'neutral': 0.947, 'sadness': 0.051, 'surprise': 0.0}}\n",
      "{'gender': 'female', 'age': 22.0, 'emotion': {'anger': 0.0, 'contempt': 0.003, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.001, 'neutral': 0.963, 'sadness': 0.032, 'surprise': 0.002}}\n",
      "{'gender': 'female', 'age': 22.0, 'emotion': {'anger': 0.0, 'contempt': 0.003, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.001, 'neutral': 0.969, 'sadness': 0.026, 'surprise': 0.001}}\n",
      "{'gender': 'female', 'age': 20.0, 'emotion': {'anger': 0.0, 'contempt': 0.001, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.001, 'neutral': 0.993, 'sadness': 0.004, 'surprise': 0.002}}\n",
      "{'gender': 'female', 'age': 21.0, 'emotion': {'anger': 0.0, 'contempt': 0.001, 'disgust': 0.0, 'fear': 0.0, 'happiness': 0.002, 'neutral': 0.986, 'sadness': 0.003, 'surprise': 0.007}}\n"
     ]
    }
   ],
   "source": [
    "#pip install opencv-python\n",
    "\n",
    "# 거울기능 + 얼굴 캡쳐 후 감정분석 코드\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "import cognitive_face as CF\n",
    "\n",
    "#노트북 카메라에서 영상을 읽어온다\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "KEY = '4416f5e6126b492ca20f2fef2dcb7a05' # 자신의 Cognitive Services API KEY\n",
    "CF.Key.set(KEY)\n",
    "\n",
    "BASE_URL = 'https://koreacentral.api.cognitive.microsoft.com/face/v1.0/' # 자신의 지역에 해당하는 URL\n",
    "CF.BaseUrl.set(BASE_URL)\n",
    "\n",
    "\n",
    "\n",
    "cnt=0\n",
    "\n",
    "#얼굴 인식 캐스케이드 파일 읽는다\n",
    "face_cascade = cv2.CascadeClassifier('C:/Users/Hong Sumin/Desktop/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "now = datetime.datetime.now().strftime(\"%d_%H-%M-%S\")\n",
    "arr=[1,2,3,4,5]\n",
    "filePath=\"C:/Users/Hong Sumin/Desktop/4-1/capture\"\n",
    "def removeAllFile(filePath):\n",
    "    if os.path.exists(filePath):\n",
    "        for file in os.scandir(filePath):\n",
    "            os.remove(file.path)\n",
    "        return \"Remove All File\"\n",
    "    else:\n",
    "        return \"Directory Not Found\"\n",
    "removeAllFile(filePath);\n",
    "\n",
    "while(True):\n",
    "    # frame 별로 capture 한다\n",
    "    ret, frame = cap.read()\n",
    "    # 좌우 반전은 1, 상하반전은 0\n",
    "    frame = cv2.flip(frame,1)\n",
    "    # 프레임이 제대로 읽어지지 않은 경우\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #detectMultiScale (InputArray image, std::vector< Rect > &objects, double scaleFactor=1.1, int minNeighbors=3, int flags=0, Size minSize=Size(), Size maxSize=Size())\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.2, 5)\n",
    "    \n",
    "    # 빨간 사각형으로 인식된 얼굴은 표시한다.\n",
    "    for i in arr:\n",
    "        if len(faces)>0:\n",
    "        \n",
    "            for (x,y,w,h) in faces:\n",
    "                #cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255),2)\n",
    "                cv2.IMREAD_UNCHANGED\n",
    "            cv2.imwrite(\"C:/Users/Hong Sumin/Desktop/4-1/capture/\" + str(i) + \".png\", frame)\n",
    "        if i==5:\n",
    "            break;\n",
    "    img_url = 'C:/Users/Hong Sumin/Desktop/4-1/capture/1.png' # 이미지 파일의 경로\n",
    "    faces = CF.face.detect(img_url,True,False,'age,gender,emotion') # 중요!\n",
    "    \n",
    "    for face in faces:\n",
    "        print(face['faceAttributes']) # 터미널 창에 속성값들을 출력\n",
    "\n",
    "\n",
    "    \n",
    "    #webCamera라는 이름으로 실시간 화면을 보여준다.\n",
    "    \n",
    "    cv2.imshow('webCamera',frame)\n",
    "    # q를 누르면 종료되도록 하는 코드이다.\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "# 메모리를 해제시켜준다.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
